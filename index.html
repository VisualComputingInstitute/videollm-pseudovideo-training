<!DOCTYPE html>
<html data-theme="light">

<head>
  <meta charset="utf-8" />
  <meta name="description"
    conjtent="" />
  <meta name="keywords"
    content="VideoLLMs, Pseudo video training, Temporal Reasoning in Video LLMs" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    How Important are Videos for Training Video LLMs?
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
  <link rel="icon" href="./static/images/favicon.svg" />

  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              How Important are Videos for Training Video LLMs?
            </h1>
            <!-- <div class="title is-4 publication-title">
                VENUE</span>
              </div> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ka.codes">George Lydakis</a><sup>1</sup></span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=V0iMeYsAAAAJ">Alexander Hermans</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.aliathar.net/">Ali Athar</a><sup>2</sup>
              </span>              
              <span class="author-block">
                <a href="https://daandegeus.com">Daan de Geus</a><sup>1,3</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ZcULDB0AAAAJ">Bastian Leibe</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>RWTH Aachen University</span>
              <span class="author-block"><sup>3</sup>ByteDance Seed</span>              
              <span class="author-block"><sup>3</sup>Eindhoven University of Technology</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- arXiv PDF. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2503.18944" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- arXiv abstract. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.18944" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- GitHub. -->
                <span class="link-block">
                  <a href="https://github.com/VisualComputingInstitute/videollm-pseudovideo-training"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (soon)</span>
                  </a>
                </span>

              </div>
            </div>

            <div class="image-container has-text-centered">
              <img src="static/img/method.webp" alt="Teaser Image" style="max-width: 100%; height: auto" />
              <figcaption style="font-size: 0.9em; margin-top: 10px; text-align: left">
                A comparison of the standard video-based training  scheme for video 
                LLMs (top) and our proposed pseudo video training scheme (bottom). 
                We utilize captioned image datasets to automatically generate short 
                pseudo videos and questions for training.                
              </figcaption>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered is-6">
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Research in Video Large Language Models (LLMs) has progressed rapidly, 
              with multiple models and benchmarks released in the span of a few years.
              Typically, these models are initialized with a pretrained text LLM and 
              are frequently finetuned on both image- and video-caption datasets.
              In this paper, we present findings indicating that Video LLMs are 
              more capable of temporal reasoning after image-only training than 
              one would assume, and that improvements from video training are 
              surprisingly small. Specifically, we show that image-trained versions 
              of two LLMs trained with the recently released LongVU algorithm perform 
              significantly above chance level on TVBench, a temporal reasoning benchmark.
              Moreover, we introduce a simple finetuning scheme involving sequences 
              of annotated images and questions targeting temporal capabilities.
              This baseline results in accuracies close to, and occasionally higher 
              than, those achieved by video-trained LLMs. This indicates suboptimal 
              utilization of rich temporal features found in real video by current models. 
              Our analysis motivates further research into the mechanisms that allow 
              image-trained LLMs to perform temporal reasoning, as well as into the 
              bottlenecks that render current video training schemes inefficient.
            </p>
          </div>
        </div>
        <!-- <div class="column has-text-centered">
          <figure style="max-width: 100%; margin: 0 auto">
            <img src="static/img/teaser.webp" alt="Inference Pipeline" style="width: 100%; height: auto" />
            <figcaption style="font-size: 0.9em; margin-top: 10px; text-align: left">
              <strong>DITR (a) and D-DITR (b).</strong> In addition to our
              DITR injection approach, we also present D-DITR to distill
              DINOv2 features into 3D semantic segmentation models.
            </figcaption>
          </figure>
        </div> -->
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-4">Quantitative Results</h2>
        </div>
      </div>
      <div class="columns is-centered is-6">
        <div class="column has-text-centered">
          <figure style="max-width: 100%; margin: 0 auto">
            <img src="static/img/table1.webp" />
          </figure>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <div style="position: relative">
        <pre><code id="bibtex">@article{abouzeid2025ditr,
  title   = {{DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation}},
  author  = {Abou Zeid, Karim and Yilmaz, Kadir and de Geus, Daan and Hermans, Alexander and Adrian, David and Linder, Timm and Leibe, Bastian},
  journal = {arXiv preprint arXiv:2503.18944},
  year    = {2025}
}</code></pre>
        <button onclick="copyToClipboard()" style="
              position: absolute;
              top: 10px;
              right: 10px;
              background-color: #f3f4f6;
              border: 1px solid #d1d5db;
              border-radius: 5px;
              padding: 5px 10px;
              cursor: pointer;
            ">
          <span style="display: flex; align-items: center; gap: 5px">
            <i class="far fa-copy has-text-grey" id="bibtex-copy-icon"></i>
            Copy
          </span>
        </button>
      </div>
      <script>
        function copyToClipboard() {
          const codeBlock = document.getElementById("bibtex").innerText;
          navigator.clipboard.writeText(codeBlock).then(
            () => {
              const icon = document.getElementById("bibtex-copy-icon");
              icon.classList.remove("far", "fa-copy", "has-text-grey");
              icon.classList.add("fas", "fa-check", "has-text-success");
            },
            (err) => alert("Failed to copy: " + err)
          );
        }
      </script>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              This website is licensed under
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>. It is based on
              the
              <a href="https://nerfies.github.io/">Nerfies website</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
